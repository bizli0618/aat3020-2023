{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45a2da87"
      },
      "source": [
        "# Named Entity Recognition\n",
        "- For a given word and its context window, estimate whether the given word is location or not"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11619f5d"
      },
      "source": [
        "# 1. Download dataset\n",
        "- CoNLL2003 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "vscode": {
          "languageId": "python"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10a4faa3",
        "outputId": "191fd2d3-5dc7-432b-8a5c-d65f5269703e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-23 06:30:19--  https://data.deepai.org/conll2003.zip\n",
            "Resolving data.deepai.org (data.deepai.org)... 185.93.1.250, 2400:52e0:1a00::1068:1\n",
            "Connecting to data.deepai.org (data.deepai.org)|185.93.1.250|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 982975 (960K) [application/zip]\n",
            "Saving to: ‘conll2003.zip’\n",
            "\n",
            "conll2003.zip       100%[===================>] 959.94K  4.17MB/s    in 0.2s    \n",
            "\n",
            "2023-03-23 06:30:20 (4.17 MB/s) - ‘conll2003.zip’ saved [982975/982975]\n",
            "\n",
            "Archive:  conll2003.zip\n",
            "  inflating: metadata                \n",
            "  inflating: test.txt                \n",
            "  inflating: train.txt               \n",
            "  inflating: valid.txt               \n"
          ]
        }
      ],
      "source": [
        "!wget https://data.deepai.org/conll2003.zip # Download dataset\n",
        "!unzip conll2003.zip # Unzip dataset zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7643dde5"
      },
      "source": [
        "## 2. Preprocess Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "vscode": {
          "languageId": "python"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d31874b2",
        "outputId": "70e4f021-0d58-4b5d-d21e-336e602881e8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['-DOCSTART- -X- -X- O',\n",
              " '',\n",
              " 'EU NNP B-NP B-ORG',\n",
              " 'rejects VBZ B-VP O',\n",
              " 'German JJ B-NP B-MISC',\n",
              " 'call NN I-NP O',\n",
              " 'to TO B-VP O',\n",
              " 'boycott VB I-VP O',\n",
              " 'British JJ B-NP B-MISC',\n",
              " 'lamb NN I-NP O',\n",
              " '. . O O',\n",
              " '',\n",
              " 'Peter NNP B-NP B-PER',\n",
              " 'Blackburn NNP I-NP I-PER',\n",
              " '',\n",
              " 'BRUSSELS NNP B-NP B-LOC',\n",
              " '1996-08-22 CD I-NP O',\n",
              " '',\n",
              " 'The DT B-NP O',\n",
              " 'European NNP I-NP B-ORG',\n",
              " 'Commission NNP I-NP I-ORG',\n",
              " 'said VBD B-VP O',\n",
              " 'on IN B-PP O',\n",
              " 'Thursday NNP B-NP O',\n",
              " 'it PRP B-NP O',\n",
              " 'disagreed VBD B-VP O',\n",
              " 'with IN B-PP O',\n",
              " 'German JJ B-NP B-MISC',\n",
              " 'advice NN I-NP O',\n",
              " 'to TO B-PP O',\n",
              " 'consumers NNS B-NP O',\n",
              " 'to TO B-VP O',\n",
              " 'shun VB I-VP O',\n",
              " 'British JJ B-NP B-MISC',\n",
              " 'lamb NN I-NP O',\n",
              " 'until IN B-SBAR O',\n",
              " 'scientists NNS B-NP O',\n",
              " 'determine VBP B-VP O',\n",
              " 'whether IN B-SBAR O',\n",
              " 'mad JJ B-NP O',\n",
              " 'cow NN I-NP O',\n",
              " 'disease NN I-NP O',\n",
              " 'can MD B-VP O',\n",
              " 'be VB I-VP O',\n",
              " 'transmitted VBN I-VP O',\n",
              " 'to TO B-PP O',\n",
              " 'sheep NN B-NP O',\n",
              " '. . O O',\n",
              " '',\n",
              " 'Germany NNP B-NP B-LOC',\n",
              " \"'s POS B-NP O\",\n",
              " 'representative NN I-NP O',\n",
              " 'to TO B-PP O',\n",
              " 'the DT B-NP O',\n",
              " 'European NNP I-NP B-ORG',\n",
              " 'Union NNP I-NP I-ORG',\n",
              " \"'s POS B-NP O\",\n",
              " 'veterinary JJ I-NP O',\n",
              " 'committee NN I-NP O',\n",
              " 'Werner NNP I-NP B-PER',\n",
              " 'Zwingmann NNP I-NP I-PER',\n",
              " 'said VBD B-VP O',\n",
              " 'on IN B-PP O',\n",
              " 'Wednesday NNP B-NP O',\n",
              " 'consumers NNS I-NP O',\n",
              " 'should MD B-VP O',\n",
              " 'buy VB I-VP O',\n",
              " 'sheepmeat NN B-NP O',\n",
              " 'from IN B-PP O',\n",
              " 'countries NNS B-NP O']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "with open(\"train.txt\") as f:\n",
        "  string = ''.join(f.readlines())\n",
        "dataset = string.split('\\n')\n",
        "\n",
        "dataset[:70]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "scrolled": true,
        "vscode": {
          "languageId": "python"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49e7f34b",
        "outputId": "d02a2ebd-5c0b-455a-f50c-e284bd5f386d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['-DOCSTART- -X- -X- O'],\n",
              " ['EU NNP B-NP B-ORG',\n",
              "  'rejects VBZ B-VP O',\n",
              "  'German JJ B-NP B-MISC',\n",
              "  'call NN I-NP O',\n",
              "  'to TO B-VP O',\n",
              "  'boycott VB I-VP O',\n",
              "  'British JJ B-NP B-MISC',\n",
              "  'lamb NN I-NP O',\n",
              "  '. . O O'],\n",
              " ['Peter NNP B-NP B-PER', 'Blackburn NNP I-NP I-PER'],\n",
              " ['BRUSSELS NNP B-NP B-LOC', '1996-08-22 CD I-NP O'],\n",
              " ['The DT B-NP O',\n",
              "  'European NNP I-NP B-ORG',\n",
              "  'Commission NNP I-NP I-ORG',\n",
              "  'said VBD B-VP O',\n",
              "  'on IN B-PP O',\n",
              "  'Thursday NNP B-NP O',\n",
              "  'it PRP B-NP O',\n",
              "  'disagreed VBD B-VP O',\n",
              "  'with IN B-PP O',\n",
              "  'German JJ B-NP B-MISC',\n",
              "  'advice NN I-NP O',\n",
              "  'to TO B-PP O',\n",
              "  'consumers NNS B-NP O',\n",
              "  'to TO B-VP O',\n",
              "  'shun VB I-VP O',\n",
              "  'British JJ B-NP B-MISC',\n",
              "  'lamb NN I-NP O',\n",
              "  'until IN B-SBAR O',\n",
              "  'scientists NNS B-NP O',\n",
              "  'determine VBP B-VP O',\n",
              "  'whether IN B-SBAR O',\n",
              "  'mad JJ B-NP O',\n",
              "  'cow NN I-NP O',\n",
              "  'disease NN I-NP O',\n",
              "  'can MD B-VP O',\n",
              "  'be VB I-VP O',\n",
              "  'transmitted VBN I-VP O',\n",
              "  'to TO B-PP O',\n",
              "  'sheep NN B-NP O',\n",
              "  '. . O O']]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from itertools import groupby\n",
        "\n",
        "dataset_in_sentence = [list(group) for k, group in groupby(dataset, lambda x: x == \"\") if not k]\n",
        "dataset_in_sentence[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "vscode": {
          "languageId": "python"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65c3dfa8",
        "outputId": "eaea5f08-bed1-4f36-8bac-7e5180c70521"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10625"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# [len(sentence) for sentence in dataset_in_sentence]\n",
        "filtered_dataset = [sentence for sentence in dataset_in_sentence if len(sentence) > 5]\n",
        "len(filtered_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "vscode": {
          "languageId": "python"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1a1714b",
        "outputId": "964abfbf-fff8-4940-9848-b68be71bd2ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EU B-ORG\n",
            "['<pad>', '<pad>', 'EU', 'rejects', 'German']\n",
            "rejects O\n",
            "['<pad>', 'EU', 'rejects', 'German', 'call']\n",
            "German B-MISC\n",
            "['EU', 'rejects', 'German', 'call', 'to']\n",
            "call O\n",
            "['rejects', 'German', 'call', 'to', 'boycott']\n",
            "to O\n",
            "['German', 'call', 'to', 'boycott', 'British']\n",
            "boycott O\n",
            "['call', 'to', 'boycott', 'British', 'lamb']\n",
            "British B-MISC\n",
            "['to', 'boycott', 'British', 'lamb', '.']\n",
            "lamb O\n",
            "['boycott', 'British', 'lamb', '.', '<pad>']\n",
            ". O\n",
            "['British', 'lamb', '.', '<pad>', '<pad>']\n"
          ]
        }
      ],
      "source": [
        "window_len = 2\n",
        "sentence = filtered_dataset[0]\n",
        "\n",
        "for i, word in enumerate(sentence):\n",
        "  # print(word)\n",
        "  splitted_word = word.split(' ')\n",
        "  # print(splitted_word)\n",
        "  center_word = splitted_word[0]\n",
        "  label = splitted_word[-1]\n",
        "  print(center_word, label)\n",
        "  is_organization = label in ['B-ORG', 'I-ORG']\n",
        "  # print(is_organization)\n",
        "  \n",
        "  prev_index = max(i - window_len, 0)\n",
        "  prev_words = sentence[prev_index:i]\n",
        "  prev_words = [word_str.split(' ')[0] for word_str in prev_words]\n",
        "\n",
        "  # print(prev_words)\n",
        "\n",
        "  next_index = i + window_len + 1\n",
        "  next_words = sentence[i+1:next_index]\n",
        "  # next_words = [sentence[next_index] ]\n",
        "  next_words = [word_str.split(' ')[0] for word_str in next_words]\n",
        "\n",
        "  # We have to add padding, if number of prev words or next words are shorter than expected\n",
        "  if len(prev_words) != window_len:\n",
        "    prev_words = ['<pad>'] * (window_len - len(prev_words)) + prev_words\n",
        "\n",
        "  if len(next_words) != window_len:\n",
        "    next_words = next_words + ['<pad>'] * (window_len - len(next_words))\n",
        "\n",
        "  concatenated_words = prev_words + [center_word] + next_words\n",
        "  print(concatenated_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader\n",
        "\n",
        "wrd2vec = gensim.downloader.load(\"glove-wiki-gigaword-300\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPO83i3UKV8C",
        "outputId": "580dcf5c-9381-4574-eb43-939e52669009"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[=================================================-] 99.9% 375.6/376.1MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(wrd2vec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtVJXmXnLoN8",
        "outputId": "3af24ee2-bf4b-4004-f83f-211a17570634"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "400000"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "vec_dim = 300\n",
        "\n",
        "corresp_vectors = []\n",
        "for word in concatenated_words:\n",
        "  if word.lower() in wrd2vec:\n",
        "    vec = wrd2vec[word.lower()]\n",
        "    # vec = torch.tensor(vec)\n",
        "  else:\n",
        "    # vec = torch.zeros(vec_dim)\n",
        "    vec = np.zeros(vec_dim)\n",
        "  corresp_vectors.append(vec)\n",
        "\n",
        "# cat_vector = torch.cat(corresp_vectors)\n",
        "cat_vector = torch.tensor(np.concatenate(corresp_vectors), dtype=torch.float).dtype\n",
        "\n",
        "cat_vector.shape\n",
        "\n",
        "# torch.tensor(np.concatenate(corresp_vectors), dtype=torch.float).dtype"
      ],
      "metadata": {
        "id": "ZwM-cn5YLPgF"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corresp_vectors[0].dtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pc_ss7HpP6Nt",
        "outputId": "4e0833a6-475a-4c91-c95e-361a5d6579a8"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('float32')"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cat_vector.dtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phiHeIOAP-uf",
        "outputId": "c6425215-2cc2-41f9-a9e9-c41b82ffe588"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.float32"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cat_vector, is_organization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wO8epL-1NaT_",
        "outputId": "edbdc157-9b85-4070-d495-8fdbe6bd965a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 0.4436, -0.2418,  0.2366,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        dtype=torch.float64), False)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Design Model"
      ],
      "metadata": {
        "id": "dc55cq4wNrgt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class OrgClassifier(nn.Module):\n",
        "  def __init__(self, input_dim=1500, hidden_size=32):\n",
        "    super().__init__()\n",
        "    self.layer1 = nn.Linear(in_features=input_dim, out_features=hidden_size)\n",
        "    self.layer2 = nn.Linear(in_features=hidden_size, out_features=1)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    hidden = self.layer1(x)\n",
        "    hidden = torch.relu(hidden)\n",
        "    out = self.layer2(hidden)\n",
        "    return out.sigmoid()\n",
        "\n",
        "model = OrgClassifier()\n",
        "model(cat_vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h04izZDGNaOV",
        "outputId": "aec39b58-af9b-429f-c219-3b85cf4937ee"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.4865], grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hidden = model.layer1(cat_vector)\n",
        "print(hidden)\n",
        "print(hidden.shape)\n",
        "hidden = torch.relu(hidden)\n",
        "print(hidden)\n",
        "out = model.layer2(hidden)\n",
        "print(out)\n",
        "print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwN-p4MOQhgW",
        "outputId": "300598db-1bc3-4d47-9ea2-2390f7403ba2"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.1375, -0.1089, -0.1297,  0.0949,  0.2792, -0.0644, -0.1344,  0.1632,\n",
            "         0.2122,  0.0221, -0.1565,  0.1963, -0.2480,  0.0403, -0.0452, -0.2661,\n",
            "        -0.1092, -0.2058, -0.4242,  0.0146,  0.3180,  0.0561, -0.1044,  0.0235,\n",
            "        -0.0238,  0.1866, -0.0081, -0.1687, -0.0784, -0.1129,  0.0550,  0.0794],\n",
            "       grad_fn=<AddBackward0>)\n",
            "torch.Size([32])\n",
            "tensor([0.0000, 0.0000, 0.0000, 0.0949, 0.2792, 0.0000, 0.0000, 0.1632, 0.2122,\n",
            "        0.0221, 0.0000, 0.1963, 0.0000, 0.0403, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0146, 0.3180, 0.0561, 0.0000, 0.0235, 0.0000, 0.1866, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0550, 0.0794], grad_fn=<ReluBackward0>)\n",
            "torch.Size([32])\n",
            "tensor([-0.1466], grad_fn=<AddBackward0>)\n",
            "torch.Size([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "relu_hidden = torch.relu(hidden) \n",
        "relu_hidden = hidden.relu()\n",
        "\n",
        "torch.sigmoid(out) == out.sigmoid()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAdhowMHRb5l",
        "outputId": "01cefb85-6875-4fbb-d781-2b8f542f9c85"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([True])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make Dataset Class"
      ],
      "metadata": {
        "id": "fB96mYlESABp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset:\n",
        "  def __init__(self, txt_fn):\n",
        "    dataset = self.read_text_data(txt_fn)\n",
        "    dataset_in_sentence = self.group_by_sentence(dataset)\n",
        "    filtered_dataset = [sentence for sentence in dataset_in_sentence if len(sentence) > 5]\n",
        "    self.data_in_sentence = filtered_dataset\n",
        "  \n",
        "  def read_text_data(self, txt_fn):\n",
        "    with open(\"train.txt\") as f:\n",
        "      string = ''.join(f.readlines())\n",
        "    dataset = string.split('\\n')\n",
        "    return dataset\n",
        "  \n",
        "  def group_by_sentence(self, dataset):\n",
        "    dataset_in_sentence = [list(group) for k, group in groupby(dataset, lambda x: x == \"\") if not k]\n",
        "    return dataset_in_sentence\n",
        "\n",
        "  def get_windowed_words_from_sentence(self, sentence):\n",
        "    result = []\n",
        "    for i, word in enumerate(sentence):\n",
        "      splitted_word = word.split(' ')\n",
        "      center_word = splitted_word[0]\n",
        "      label = splitted_word[-1]\n",
        "      is_organization = label in ['B-ORG', 'I-ORG']\n",
        "      \n",
        "      prev_index = max(i - window_len, 0)\n",
        "      prev_words = sentence[prev_index:i]\n",
        "      prev_words = [word_str.split(' ')[0] for word_str in prev_words]\n",
        "\n",
        "\n",
        "      next_index = i + window_len + 1\n",
        "      next_words = sentence[i+1:next_index]\n",
        "      next_words = [word_str.split(' ')[0] for word_str in next_words]\n",
        "\n",
        "      # We have to add padding, if number of prev words or next words are shorter than expected\n",
        "      if len(prev_words) != window_len:\n",
        "        prev_words = ['<pad>'] * (window_len - len(prev_words)) + prev_words\n",
        "\n",
        "      if len(next_words) != window_len:\n",
        "        next_words = next_words + ['<pad>'] * (window_len - len(next_words))\n",
        "\n",
        "      concatenated_words = prev_words + [center_word] + next_words\n",
        "      result.append( (concatenated_words, is_organization))\n",
        "    return result\n",
        "\n",
        "dataset = Dataset(\"train.txt\")"
      ],
      "metadata": {
        "id": "nPGbWxVWR_h1"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.get_windowed_words_from_sentence(dataset.data_in_sentence[100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGhz5RwYR3BZ",
        "outputId": "239f12e8-51fe-4243-9b9f-b65f42d4d5b7"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(['<pad>', '<pad>', 'Israel', \"'s\", 'Channel'], False),\n",
              " (['<pad>', 'Israel', \"'s\", 'Channel', 'Two'], False),\n",
              " (['Israel', \"'s\", 'Channel', 'Two', 'television'], True),\n",
              " ([\"'s\", 'Channel', 'Two', 'television', 'said'], True),\n",
              " (['Channel', 'Two', 'television', 'said', 'Damascus'], False),\n",
              " (['Two', 'television', 'said', 'Damascus', 'had'], False),\n",
              " (['television', 'said', 'Damascus', 'had', 'sent'], False),\n",
              " (['said', 'Damascus', 'had', 'sent', 'a'], False),\n",
              " (['Damascus', 'had', 'sent', 'a', '\"'], False),\n",
              " (['had', 'sent', 'a', '\"', 'calming'], False),\n",
              " (['sent', 'a', '\"', 'calming', 'signal'], False),\n",
              " (['a', '\"', 'calming', 'signal', '\"'], False),\n",
              " (['\"', 'calming', 'signal', '\"', 'to'], False),\n",
              " (['calming', 'signal', '\"', 'to', 'Israel'], False),\n",
              " (['signal', '\"', 'to', 'Israel', '.'], False),\n",
              " (['\"', 'to', 'Israel', '.', '<pad>'], False),\n",
              " (['to', 'Israel', '.', '<pad>', '<pad>'], False)]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "nbformat": 4,
    "nbformat_minor": 5,
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}